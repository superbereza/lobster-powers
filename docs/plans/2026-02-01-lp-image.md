# lp-image Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Analyze images using vision models (OpenAI, Anthropic).

**Architecture:** Support local files, URLs, and data URLs. Base64 encoding for API calls.

**Tech Stack:** openai, anthropic, httpx, pillow (for image validation)

---

## Task 1: Create image.py with OpenAI vision

**Files:**
- Create: `src/lobster_powers/tools/image.py`
- Modify: `pyproject.toml`

**Step 1: Write implementation**

```python
#!/usr/bin/env python3
"""
lp-image: Analyze images using vision models.

Examples:
    lp-image photo.jpg
    lp-image photo.jpg "What objects are in this image?"
    lp-image https://example.com/image.png "Describe this"
    lp-image photo.jpg --provider anthropic
"""

import argparse
import base64
import mimetypes
import os
import sys
from pathlib import Path
from urllib.parse import urlparse

import httpx


DEFAULT_PROMPT = "Describe this image in detail."
DEFAULT_MAX_TOKENS = 1024


def load_image_as_base64(source: str) -> tuple[str, str]:
    """
    Load image from file path or URL, return (base64_data, mime_type).
    """
    # Data URL
    if source.startswith("data:"):
        # data:image/png;base64,xxxxx
        header, data = source.split(",", 1)
        mime_type = header.split(";")[0].split(":")[1]
        return data, mime_type

    # URL
    if source.startswith(("http://", "https://")):
        response = httpx.get(source, follow_redirects=True, timeout=30)
        response.raise_for_status()
        content_type = response.headers.get("content-type", "image/png")
        mime_type = content_type.split(";")[0]
        return base64.b64encode(response.content).decode(), mime_type

    # Local file
    path = Path(source).expanduser()
    if not path.exists():
        raise FileNotFoundError(f"Image not found: {source}")

    mime_type, _ = mimetypes.guess_type(str(path))
    if not mime_type or not mime_type.startswith("image/"):
        mime_type = "image/png"

    with open(path, "rb") as f:
        return base64.b64encode(f.read()).decode(), mime_type


def analyze_openai(
    image_base64: str,
    mime_type: str,
    prompt: str,
    api_key: str,
    model: str = "gpt-4o-mini",
) -> str:
    """Analyze image using OpenAI Vision API."""
    from openai import OpenAI

    client = OpenAI(api_key=api_key)

    response = client.chat.completions.create(
        model=model,
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:{mime_type};base64,{image_base64}"
                        },
                    },
                ],
            }
        ],
        max_tokens=DEFAULT_MAX_TOKENS,
    )

    return response.choices[0].message.content


def analyze_anthropic(
    image_base64: str,
    mime_type: str,
    prompt: str,
    api_key: str,
    model: str = "claude-sonnet-4-20250514",
) -> str:
    """Analyze image using Anthropic Vision API."""
    from anthropic import Anthropic

    client = Anthropic(api_key=api_key)

    response = client.messages.create(
        model=model,
        max_tokens=DEFAULT_MAX_TOKENS,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": mime_type,
                            "data": image_base64,
                        },
                    },
                    {"type": "text", "text": prompt},
                ],
            }
        ],
    )

    return response.content[0].text


def get_provider_and_key() -> tuple[str, str]:
    """Detect available provider and API key."""
    openai_key = os.environ.get("OPENAI_API_KEY")
    anthropic_key = os.environ.get("ANTHROPIC_API_KEY")

    if openai_key:
        return "openai", openai_key
    if anthropic_key:
        return "anthropic", anthropic_key

    return "", ""


def main():
    parser = argparse.ArgumentParser(
        description="Analyze images using vision models",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument("image", help="Image file path or URL")
    parser.add_argument("prompt", nargs="?", default=DEFAULT_PROMPT, help="Prompt/question about the image")
    parser.add_argument(
        "--provider", "-p",
        choices=["openai", "anthropic"],
        help="Vision provider (auto-detected by default)"
    )
    parser.add_argument(
        "--model", "-m",
        help="Model to use (default: gpt-4o-mini or claude-sonnet-4)"
    )

    args = parser.parse_args()

    # Get provider
    default_provider, default_key = get_provider_and_key()
    provider = args.provider or default_provider

    if not provider:
        print("Error: No API key found. Set OPENAI_API_KEY or ANTHROPIC_API_KEY", file=sys.stderr)
        sys.exit(1)

    # Get API key
    if provider == "openai":
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            print("Error: OPENAI_API_KEY not set", file=sys.stderr)
            sys.exit(1)
        model = args.model or "gpt-4o-mini"
    else:
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            print("Error: ANTHROPIC_API_KEY not set", file=sys.stderr)
            sys.exit(1)
        model = args.model or "claude-sonnet-4-20250514"

    try:
        # Load image
        image_base64, mime_type = load_image_as_base64(args.image)

        # Analyze
        if provider == "openai":
            result = analyze_openai(image_base64, mime_type, args.prompt, api_key, model)
        else:
            result = analyze_anthropic(image_base64, mime_type, args.prompt, api_key, model)

        print(result)

    except FileNotFoundError as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
```

**Step 2: Update pyproject.toml**

Add to `[project.scripts]`:
```toml
lp-image = "lobster_powers.tools.image:main"
```

Add to `[project.optional-dependencies]`:
```toml
image = ["openai", "anthropic", "httpx"]
```

**Step 3: Test manually**

```bash
pip install -e ".[image]"
# Download a test image
curl -o /tmp/test.jpg https://picsum.photos/400/300
lp-image /tmp/test.jpg
lp-image /tmp/test.jpg "What colors are in this image?"
```

**Step 4: Commit**

```bash
git add src/lobster_powers/tools/image.py pyproject.toml
git commit -m "feat(image): add lp-image with OpenAI/Anthropic vision"
```

---

## Task 2: Create skill file

**Files:**
- Create: `skills/image.md`

**Step 1: Write skill file**

```markdown
---
name: image
description: Analyze images using vision models via lp-image
---

# Image - Vision Analysis

Use `lp-image` to analyze images with AI vision models.

## Quick Examples

```bash
# Describe an image
lp-image photo.jpg

# Ask a specific question
lp-image photo.jpg "What objects are visible?"

# Analyze from URL
lp-image https://example.com/image.png "Describe this"

# Use specific provider
lp-image photo.jpg --provider anthropic
```

## Providers

| Provider | API Key | Models |
|----------|---------|--------|
| openai | `OPENAI_API_KEY` | gpt-4o-mini (default), gpt-4o |
| anthropic | `ANTHROPIC_API_KEY` | claude-sonnet-4 (default) |

## Commands

| Option | Description |
|--------|-------------|
| `--provider, -p` | openai or anthropic |
| `--model, -m` | Specific model to use |

## Supported Formats

- Local files: `photo.jpg`, `~/images/pic.png`
- URLs: `https://example.com/image.png`
- Data URLs: `data:image/png;base64,...`

## When to Use

- Describing image contents
- Extracting text from screenshots
- Analyzing charts and diagrams
- Identifying objects in photos
```

**Step 2: Commit**

```bash
git add skills/image.md
git commit -m "docs(image): add skill file"
```

---

## Summary

Total: 2 tasks, ~170 lines of code.

Estimated time: 15-20 minutes.
